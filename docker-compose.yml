# Lightweight Setup with standalone Airflow for Local Development

services:
  postgres:
    image: postgres:15-alpine
    container_name: lakepulse-oltp-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    env_file:
      - .env
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - oltp-db-volume:/var/lib/postgresql/data
      - ./data:/data
      - ./docker/postgres:/docker-entrypoint-initdb.d/
    networks:
      - lakepulse_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
  minio:
    image: minio/minio:latest
    container_name: lakepulse-minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    env_file:
      - .env
    ports:
      - "${MINIO_API_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9001}:9001"
    volumes:
      - ./data/lake:/data
    networks:
      - lakepulse_network
    command: server /data --console-address ":9001"
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
  airflow:
    build: ./docker/airflow
    image: lakepulse/airflow:latest
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
    volumes:
      - ./airflow/config:/opt/airflow/config
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./spark_jobs:/opt/spark_jobs
      - spark-jars-volume:/opt/spark/jars
      - airflow-db-volume:/opt/airflow
    networks:
      - lakepulse_network
    ports:
      - "8090:8080"
    command: standalone
  
  spark-master:
    image: lakepulse/spark:latest
    container_name: lakepulse-spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_LOCAL_IP=spark-master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
    expose:
      - "${SPARK_MASTER_PORT:-7077}"
      - "${SPARK_CONTEXT_UI_PORT:-4040}"
    ports:
      - "${SPARK_MASTER_PORT:-7077}:7077"
      - "${SPARK_MASTER_WEB_UI_PORT:-8080}:8080"
      - "${SPARK_CONTEXT_UI_PORT:-4040}:4040"
    volumes:
      - ./docker/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./spark_jobs:/opt/spark_jobs
      - ./data:/opt/bitnami/spark/data
      - spark-jars-volume:/opt/bitnami/spark/jars
    env_file:
      - .env
    networks:
      - lakepulse_network
    restart: unless-stopped
  spark-worker:
    image: lakepulse/spark:latest
    container_name: lakepulse-spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:${SPARK_MASTER_PORT:-7077}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY:-2g}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES:-2}
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
    ports:
      - "${SPARK_WORKER_WEB_UI_PORT:-8081}:8081"
    env_file:
      - .env
    depends_on:
      - spark-master
    networks:
      - lakepulse_network
    restart: unless-stopped

volumes:
  oltp-db-volume:
  airflow-db-volume:
  spark-jars-volume:

networks:
  lakepulse_network:
    driver: bridge