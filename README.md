# LakePulse

> A production-grade, fully open-source data lakehouse pipeline implementing modern data engineering patterns

LakePulse is a comprehensive data lakehouse solution that demonstrates real-world data engineering practices using industry-standard technologies. Built for local development and learning, it implements Change Data Capture (CDC) simulation, real-time data processing, and the modern Bronze-Silver-Gold medallion architecture.

**Note: This project is still in development! The documentation here reflects the estimated design.**

## Overview

LakePulse showcases a complete end-to-end data pipeline that:
- **Ingests** data from OLTP databases using simulated CDC
- **Processes** data through multiple layers (Bronze, Silver, Gold) 
- **Stores** data in Delta Lake format for ACID transactions
- **Orchestrates** workflows with Apache Airflow
- **Enables** analytics through SQL interfaces

## Architecture

![LakePulse Architecture](docs/images/architecture.png)

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Storage** | MinIO | S3-compatible object storage |
| **Data Format** | Delta Lake | ACID transactions, time travel |
| **Processing** | Apache Spark | Distributed data processing |
| **Orchestration** | Apache Airflow | Workflow management |
| **Streaming** | Apache Kafka with Schema Registry (Avro) | Change data capture simulation along with schema evolution |
| **Analytics** | Trino | Distributed SQL query engine |
| **Source DB** | PostgreSQL | OLTP database (Wide World Importers) |
| **Continous Integration** | Github Actions | Interactive data analysis |
| **Monitoring and Alerting** | Prometheus + Grafana | Interactive data analysis |

## Project Structure

```
lakepulse/
â”œâ”€â”€ ğŸ“ airflow/                 # Airflow DAGs and configuration
â”‚   â”œâ”€â”€ config/                 # Airflow configuration
â”‚   â”œâ”€â”€ dags/                   # Pipeline definitions
â”‚   â””â”€â”€ plugins/                # Airflow plugins
â”œâ”€â”€ ğŸ“ docker/                  # Service configurations
â”‚   â”œâ”€â”€ postgres/               # Postgres database init
â”‚   â”œâ”€â”€ airflow/                # Airflow Docker setup
â”‚   â”œâ”€â”€ spark/                  # Spark Docker setup
â”‚   â””â”€â”€ kafka-connect/          # Kafka connect Docker setup
â”œâ”€â”€ ğŸ“ spark_jobs/              # Data processing jobs
â”‚   â”œâ”€â”€ bronze/                 # Raw data ingestion
â”‚   â”œâ”€â”€ silver/                 # Data cleaning & validation
â”‚   â”œâ”€â”€ gold/                   # Business aggregations
â”‚   â””â”€â”€ utils/                  # Shared utilities
â”œâ”€â”€ ğŸ“ data/                    # Sample datasets
â”œâ”€â”€ ğŸ“ great_expectations/      # GE project config, expectation suites, checkpoints
â”œâ”€â”€ ğŸ“ trinio/                  # Trino catalog configs (e.g. catalog/delta.properties)
â”œâ”€â”€ ğŸ“ scripts/                 # Utility scripts
â”œâ”€â”€ ğŸ³ docker-compose.yml       # Multi-service orchestration
â”œâ”€â”€ ğŸ“‹ Makefile                 # Common commands
â””â”€â”€ ğŸ“– README.md                # This file
```

## Learning Objectives

This project demonstrates:

1. **Modern Data Architecture**: Medallion architecture with Bronze-Silver-Gold layers
2. **Real-time Processing**: Kafka-based streaming and CDC patterns  
3. **Data Lake Technologies**: Delta Lake, Parquet, and S3-compatible storage
4. **Workflow Orchestration**: Airflow DAGs for complex data pipelines
5. **Data Quality**: Validation, deduplication, and monitoring techniques
6. **Performance Optimization**: Partitioning, indexing, and query optimization
7. **DevOps Practices**: Containerization, infrastructure as code

---

â­ **Star this repository if you find it helpful!**
